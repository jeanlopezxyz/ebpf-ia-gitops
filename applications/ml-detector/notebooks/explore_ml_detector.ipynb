{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Exploración del ML Detector\n",
    "\n",
    "## Objetivo\n",
    "Este notebook explora y visualiza el funcionamiento del sistema de detección de amenazas basado en Machine Learning.\n",
    "\n",
    "## ¿Qué hace este notebook?\n",
    "- **Instancia el `ThreatDetector`**: Usa el código real del proyecto sin duplicarlo\n",
    "- **Genera datos sintéticos**: Crea muestras de tráfico normal y anómalo para entrenamiento\n",
    "- **Entrena modelos ML**: Ejecuta algoritmos KMeans, LOF y OneClassSVM\n",
    "- **Visualiza resultados**: Proyecta datos 6D a 2D usando PCA para análisis visual\n",
    "- **Evalúa detección**: Prueba la capacidad de detección con muestras de prueba\n",
    "\n",
    "## Casos de uso que simula:\n",
    "- **Tráfico normal**: Patrones típicos de red con variabilidad natural\n",
    "- **DDoS**: Alto volumen de paquetes desde pocas IPs\n",
    "- **Port Scanning**: Muchos puertos únicos desde pocas IPs  \n",
    "- **Data Exfiltration**: Alto volumen de bytes con pocos puertos\n",
    "\n",
    "## Requisitos\n",
    "Instalar dependencias de desarrollo: `pip install -r applications/ml-detector/requirements-dev.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Configuración del entorno\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Configura las rutas para importar el código del proyecto ML Detector\n",
    "- Desactiva el entrenamiento automático para tener control manual\n",
    "- Configura matplotlib para visualizaciones de alta calidad\n",
    "\n",
    "**¿Por qué es importante?**\n",
    "- Permite usar el código real del detector sin duplicarlo\n",
    "- Evita entrenamientos automáticos que podrían interferir con la exploración\n",
    "- Asegura que las visualizaciones se muestren correctamente en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar ruta para importar módulos del proyecto\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Asume que este notebook vive en applications/ml-detector/notebooks\n",
    "nb_dir = Path.cwd()\n",
    "app_dir = nb_dir.parent  # applications/ml-detector\n",
    "sys.path.insert(0, str(app_dir))\n",
    "\n",
    "# Desactivar entrenamiento/baseline automáticos para exploración reproducible\n",
    "# Esto permite controlar manualmente cuándo entrenar los modelos\n",
    "os.environ['TRAINING_ENABLED'] = 'false'\n",
    "os.environ['BASELINE_ENABLED'] = 'false'\n",
    "os.environ.setdefault('MODEL_PATH', '/tmp/models')\n",
    "\n",
    "# Configurar matplotlib para visualizaciones de alta calidad\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-section",
   "metadata": {},
   "source": [
    "## 2. Inicialización del detector\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Importa las librerías necesarias para análisis de datos y visualización\n",
    "- Crea una instancia del `ThreatDetector` real del proyecto\n",
    "- Verifica el estado inicial del detector (sin entrenar)\n",
    "\n",
    "**Librerías utilizadas:**\n",
    "- `numpy`: Operaciones numéricas y matrices\n",
    "- `matplotlib.pyplot`: Gráficos y visualizaciones\n",
    "- `seaborn`: Visualizaciones estadísticas avanzadas\n",
    "- `sklearn.decomposition.PCA`: Reducción de dimensionalidad\n",
    "- `detector.ThreatDetector`: Clase principal del sistema de detección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el detector y dependencias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Importar la clase principal del sistema de detección\n",
    "from detector import ThreatDetector\n",
    "\n",
    "# Crear instancia del detector (inicialmente sin entrenar)\n",
    "detector = ThreatDetector()\n",
    "print(f\"¿Detector entrenado? {detector.is_trained}\")\n",
    "\n",
    "# El detector maneja automáticamente la inicialización de modelos:\n",
    "# - KMeans para clustering de patrones normales\n",
    "# - LocalOutlierFactor (LOF) para detección de outliers\n",
    "# - OneClassSVM para clasificación de una clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-generation-section",
   "metadata": {},
   "source": [
    "## 3. Generación de datos sintéticos\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Crea funciones para generar datos de tráfico de red sintéticos\n",
    "- Simula patrones de tráfico normal y diferentes tipos de amenazas\n",
    "- Utiliza distribuciones estadísticas realistas para cada tipo de tráfico\n",
    "\n",
    "**Tipos de tráfico simulado:**\n",
    "\n",
    "### Tráfico Normal\n",
    "- **Paquetes/segundo**: ~50 (distribución normal)\n",
    "- **Bytes/segundo**: ~50,000 (distribución normal)\n",
    "- **IPs únicas**: 5-20 (variedad típica)\n",
    "- **Puertos únicos**: 3-10 (aplicaciones comunes)\n",
    "- **Ratio TCP**: 60-80% (protocolo predominante)\n",
    "- **Paquetes SYN**: 10-50 (conexiones normales)\n",
    "\n",
    "### Amenazas Simuladas\n",
    "1. **DDoS**: Alto volumen de paquetes, pocas IPs origen\n",
    "2. **Port Scanning**: Muchos puertos únicos, pocas IPs\n",
    "3. **Data Exfiltration**: Alto volumen de bytes, pocos puertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos: tráfico normal y anomalías\n",
    "\n",
    "def gen_normal(n=300, seed=42):\n",
    "    \"\"\"\n",
    "    Genera datos de tráfico de red normal.\n",
    "    \n",
    "    Simula patrones típicos de red empresarial con:\n",
    "    - Volumen moderado de tráfico\n",
    "    - Variabilidad natural en las métricas\n",
    "    - Distribuciones gaussianas para realismo\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return [\n",
    "        {\n",
    "            'packets_per_second': float(rng.normal(50, 10)),      # Tráfico moderado\n",
    "            'bytes_per_second': float(rng.normal(50000, 10000)),  # ~50KB/s típico\n",
    "            'unique_ips': int(rng.integers(5, 20)),               # Diversidad normal\n",
    "            'unique_ports': int(rng.integers(3, 10)),             # Aplicaciones comunes\n",
    "            'tcp_ratio': float(rng.uniform(0.6, 0.8)),            # TCP predominante\n",
    "            'syn_packets': int(rng.integers(10, 50)),             # Conexiones normales\n",
    "        } for _ in range(n)\n",
    "    ]\n",
    "\n",
    "def gen_anomalies(n=60, seed=123):\n",
    "    \"\"\"\n",
    "    Genera datos de tráfico anómalo que representan diferentes amenazas.\n",
    "    \n",
    "    Tipos de amenazas simuladas:\n",
    "    - Modo 0 (DDoS): Alto volumen de paquetes, pocas IPs\n",
    "    - Modo 1 (Port Scan): Muchos puertos, pocas IPs  \n",
    "    - Modo 2 (Data Exfil): Alto volumen de bytes, pocos puertos\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        mode = rng.integers(0, 3)  # Seleccionar tipo de amenaza aleatoriamente\n",
    "        \n",
    "        if mode == 0:  # DDoS-like: volumen extremo, origen concentrado\n",
    "            d = {\n",
    "                'packets_per_second': float(rng.normal(2000, 200)),    # 40x más paquetes\n",
    "                'bytes_per_second': float(rng.normal(2e6, 2e5)),       # 40x más bytes\n",
    "                'unique_ips': int(rng.integers(1, 5)),                 # Pocas IPs origen\n",
    "                'unique_ports': int(rng.integers(1, 3)),               # Pocos puertos\n",
    "                'tcp_ratio': float(rng.uniform(0.7, 1.0)),             # Alto TCP\n",
    "                'syn_packets': int(rng.integers(500, 1200))            # Muchos SYN\n",
    "            }\n",
    "        elif mode == 1:  # Port-scan-like: exploración de puertos\n",
    "            d = {\n",
    "                'packets_per_second': float(rng.normal(200, 30)),      # Tráfico moderado\n",
    "                'bytes_per_second': float(rng.normal(1e5, 2e4)),       # Bytes moderados\n",
    "                'unique_ips': int(rng.integers(1, 3)),                 # Pocas IPs origen\n",
    "                'unique_ports': int(rng.integers(30, 200)),            # MUCHOS puertos\n",
    "                'tcp_ratio': float(rng.uniform(0.4, 0.9)),             # TCP variable\n",
    "                'syn_packets': int(rng.integers(50, 150))              # SYN moderado\n",
    "            }\n",
    "        else:  # Exfiltration-like: transferencia masiva de datos\n",
    "            d = {\n",
    "                'packets_per_second': float(rng.normal(120, 20)),      # Tráfico moderado\n",
    "                'bytes_per_second': float(rng.normal(6e6, 5e5)),       # MUCHOS bytes\n",
    "                'unique_ips': int(rng.integers(1, 4)),                 # Pocas IPs\n",
    "                'unique_ports': int(rng.integers(1, 5)),               # Pocos puertos\n",
    "                'tcp_ratio': float(rng.uniform(0.85, 1.0)),            # Casi todo TCP\n",
    "                'syn_packets': int(rng.integers(20, 60))               # SYN normal\n",
    "            }\n",
    "        data.append(d)\n",
    "    return data\n",
    "\n",
    "# Generar datasets: 400 muestras normales + 80 anomalías\n",
    "normal = gen_normal(400)\n",
    "anoms = gen_anomalies(80)\n",
    "print(f\"Generados: {len(normal)} muestras normales, {len(anoms)} anomalías\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del detector\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Alimenta al detector con datos de tráfico normal únicamente\n",
    "- Entrena los modelos de machine learning usando patrones normales\n",
    "- Verifica que el entrenamiento fue exitoso\n",
    "\n",
    "**Proceso de entrenamiento:**\n",
    "1. **Extracción de características**: Convierte datos crudos en vectores numéricos\n",
    "2. **Normalización**: Escala las características para optimizar el rendimiento\n",
    "3. **Entrenamiento de modelos**: Ajusta KMeans, LOF y OneClassSVM\n",
    "4. **Validación**: Confirma que los modelos están listos para detección\n",
    "\n",
    "**¿Por qué solo datos normales?**\n",
    "Los algoritmos de detección de anomalías aprenden qué es \"normal\" y detectan desviaciones de estos patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alimentar datos normales al detector y entrenar modelos\n",
    "\n",
    "print(\"Procesando datos normales...\")\n",
    "# El detector extrae características y las almacena internamente\n",
    "for i, d in enumerate(normal):\n",
    "    detector.extract_features(d)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Procesadas {i + 1}/{len(normal)} muestras\")\n",
    "\n",
    "print(\"\\nEntrenando modelos de machine learning...\")\n",
    "# Entrena KMeans, LOF y OneClassSVM con los datos normales acumulados\n",
    "detector.train_models()\n",
    "\n",
    "print(f\"✅ Entrenamiento completado. Detector entrenado: {detector.is_trained}\")\n",
    "\n",
    "# Verificar que los modelos fueron entrenados correctamente\n",
    "print(f\"📊 Muestras de entrenamiento: {len(detector.features)}\")\n",
    "print(f\"🎯 Clusters KMeans: {detector.kmeans.n_clusters}\")\n",
    "print(f\"🔍 Vecinos LOF: {detector.lof.n_neighbors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep-section",
   "metadata": {},
   "source": [
    "## 5. Preparación de datos para visualización\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Convierte los datos de tráfico a matrices numéricas para análisis\n",
    "- Combina datos normales y anómalos en un dataset unificado\n",
    "- Aplica PCA para reducir dimensiones de 6D a 2D para visualización\n",
    "\n",
    "**Vector de características (6 dimensiones):**\n",
    "1. `packets_per_second`: Volumen de paquetes\n",
    "2. `bytes_per_second`: Volumen de datos  \n",
    "3. `unique_ips`: Diversidad de direcciones IP\n",
    "4. `unique_ports`: Diversidad de puertos\n",
    "5. `tcp_ratio`: Proporción de tráfico TCP\n",
    "6. `syn_packets`: Número de paquetes de inicio de conexión\n",
    "\n",
    "**¿Por qué PCA?**\n",
    "PCA (Principal Component Analysis) nos permite visualizar datos multidimensionales en un gráfico 2D manteniendo la mayor variabilidad posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-prep-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir datos a matrices numéricas y aplicar PCA\n",
    "\n",
    "def to_vec(d):\n",
    "    \"\"\"Convierte un diccionario de métricas de tráfico a vector numérico.\"\"\"\n",
    "    return [\n",
    "        d.get('packets_per_second', 0),    # Dimensión 0: Volumen de paquetes\n",
    "        d.get('bytes_per_second', 0),      # Dimensión 1: Volumen de bytes\n",
    "        d.get('unique_ips', 0),            # Dimensión 2: Diversidad de IPs\n",
    "        d.get('unique_ports', 0),          # Dimensión 3: Diversidad de puertos\n",
    "        d.get('tcp_ratio', 0.5),           # Dimensión 4: Proporción TCP\n",
    "        d.get('syn_packets', 0),           # Dimensión 5: Paquetes SYN\n",
    "    ]\n",
    "\n",
    "# Crear matrices de datos\n",
    "Xn = np.array([to_vec(d) for d in normal])    # 400 x 6: datos normales\n",
    "Xa = np.array([to_vec(d) for d in anoms])     # 80 x 6: datos anómalos\n",
    "X = np.vstack([Xn, Xa])                       # 480 x 6: dataset completo\n",
    "y = np.array([0]*len(Xn) + [1]*len(Xa))       # Etiquetas: 0=normal, 1=anómalo\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Normal: {len(Xn)} muestras, Anómalos: {len(Xa)} muestras\")\n",
    "\n",
    "# Aplicar escalado usando el scaler del detector entrenado\n",
    "# Esto asegura consistencia con el modelo en producción\n",
    "Xs = detector.scaler.transform(X) if hasattr(detector.scaler, 'mean_') else X\n",
    "\n",
    "# Proyección PCA: 6D → 2D para visualización\n",
    "# PCA encuentra las 2 direcciones de mayor variabilidad en los datos\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "Z = pca.fit_transform(Xs)\n",
    "\n",
    "print(f\"Datos proyectados a 2D: {Z.shape}\")\n",
    "print(f\"Varianza explicada por PC1: {pca.explained_variance_ratio_[0]:.3f}\")\n",
    "print(f\"Varianza explicada por PC2: {pca.explained_variance_ratio_[1]:.3f}\")\n",
    "print(f\"Varianza total explicada: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Mostrar primeras muestras proyectadas\n",
    "print(f\"\\nPrimeras 3 muestras en espacio PCA:\")\n",
    "print(Z[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-basic-section",
   "metadata": {},
   "source": [
    "## 6. Visualización: Distribución de datos normales vs anómalos\n",
    "\n",
    "**¿Qué hace esta visualización?**\n",
    "- Muestra la separación entre tráfico normal y anómalo en el espacio 2D (PCA)\n",
    "- Usa colores para distinguir: azul = normal, rojo = anómalo\n",
    "- Permite evaluar visualmente qué tan separables son los patrones\n",
    "\n",
    "**¿Qué esperamos ver?**\n",
    "- **Datos normales (azul)**: Agrupados en una región compacta\n",
    "- **Datos anómalos (rojo)**: Dispersos en regiones alejadas del cluster normal\n",
    "- **Separación clara**: Indica que los algoritmos ML podrán distinguir amenazas\n",
    "\n",
    "**Interpretación:**\n",
    "- Si hay solapamiento significativo → detección más difícil\n",
    "- Si hay separación clara → detección más confiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-basic-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: distribución de datos normales vs anómalos\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Crear scatter plot con colores diferenciados\n",
    "sns.scatterplot(x=Z[:,0], y=Z[:,1], hue=y, \n",
    "               palette={0: 'tab:blue', 1: 'tab:red'}, \n",
    "               s=30, alpha=0.7)\n",
    "\n",
    "# Personalizar el gráfico\n",
    "plt.title('Proyección PCA: Tráfico Normal vs Amenazas', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)', fontsize=12)\n",
    "\n",
    "# Personalizar leyenda\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, ['Normal', 'Amenaza'], title='Tipo de Tráfico', \n",
    "          title_fontsize=12, fontsize=11)\n",
    "\n",
    "# Agregar grid para mejor lectura\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de separación\n",
    "normal_center = Z[y==0].mean(axis=0)\n",
    "anomaly_center = Z[y==1].mean(axis=0)\n",
    "separation_distance = np.linalg.norm(normal_center - anomaly_center)\n",
    "\n",
    "print(f\"📍 Centro datos normales: PC1={normal_center[0]:.2f}, PC2={normal_center[1]:.2f}\")\n",
    "print(f\"📍 Centro datos anómalos: PC1={anomaly_center[0]:.2f}, PC2={anomaly_center[1]:.2f}\")\n",
    "print(f\"📏 Distancia de separación: {separation_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans-section",
   "metadata": {},
   "source": [
    "## 7. Análisis de clustering con KMeans\n",
    "\n",
    "**¿Qué hace esta visualización?**\n",
    "- Muestra cómo KMeans agrupa los datos en clusters\n",
    "- Visualiza los centros de los clusters como marcas X negras\n",
    "- Colorea cada punto según su asignación de cluster\n",
    "\n",
    "**¿Cómo funciona KMeans en el detector?**\n",
    "1. **Entrenamiento**: Solo con datos normales, identifica patrones comunes\n",
    "2. **Clustering**: Agrupa datos similares en K clusters (típicamente K=3-5)\n",
    "3. **Detección**: Calcula distancia de nuevas muestras a centros de clusters\n",
    "4. **Umbral**: Si distancia > umbral → posible amenaza\n",
    "\n",
    "**¿Qué buscamos?**\n",
    "- **Datos normales**: Cerca de los centros de clusters\n",
    "- **Datos anómalos**: Lejos de todos los centros de clusters\n",
    "- **Centros**: Representan los patrones típicos de tráfico normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de clusters KMeans y sus centros\n",
    "\n",
    "# Obtener asignaciones de cluster para todos los datos\n",
    "labels = detector.kmeans.predict(Xs)\n",
    "\n",
    "# Proyectar centros de clusters de 6D a 2D para visualización\n",
    "centers_6d = detector.kmeans.cluster_centers_  # Centros en espacio original\n",
    "centers_2d = pca.transform(centers_6d)         # Centros proyectados a PCA\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot coloreado por cluster asignado\n",
    "sns.scatterplot(x=Z[:,0], y=Z[:,1], hue=labels, \n",
    "               palette='tab10', s=25, alpha=0.7, legend=False)\n",
    "\n",
    "# Marcar centros de clusters\n",
    "plt.scatter(centers_2d[:,0], centers_2d[:,1], \n",
    "           c='black', s=200, marker='x', linewidths=3,\n",
    "           label='Centros de Clusters', zorder=5)\n",
    "\n",
    "# Personalizar gráfico\n",
    "plt.title('KMeans: Clustering de Patrones de Tráfico', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de cluster assignments\n",
    "print(\"📊 Análisis de asignaciones de cluster:\")\n",
    "for cluster_id in range(detector.kmeans.n_clusters):\n",
    "    cluster_mask = labels == cluster_id\n",
    "    normal_in_cluster = np.sum(cluster_mask[:len(Xn)])\n",
    "    anomaly_in_cluster = np.sum(cluster_mask[len(Xn):])\n",
    "    total_in_cluster = np.sum(cluster_mask)\n",
    "    \n",
    "    print(f\"Cluster {cluster_id}: {total_in_cluster} muestras \"\n",
    "          f\"({normal_in_cluster} normales, {anomaly_in_cluster} anómalas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmaps-section",
   "metadata": {},
   "source": [
    "## 8. Mapas de calor: LOF y One-Class SVM\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Crea mapas de calor mostrando las \"puntuaciones de decisión\" de cada algoritmo\n",
    "- Visualiza cómo cada modelo ve el espacio de características\n",
    "- Compara las regiones que cada algoritmo considera normales vs anómalas\n",
    "\n",
    "**Local Outlier Factor (LOF):**\n",
    "- **Función**: Detecta outliers basándose en densidad local\n",
    "- **Interpretación**: Valores positivos = normal, negativos = outlier\n",
    "- **Visualización**: Mapa de calor donde rojo = más anómalo\n",
    "\n",
    "**One-Class SVM:**\n",
    "- **Función**: Crea una frontera alrededor de datos normales\n",
    "- **Interpretación**: Valores positivos = dentro de la frontera (normal)\n",
    "- **Visualización**: Mapa de calor donde verde = normal, púrpura = anómalo\n",
    "\n",
    "**¿Cómo leer los mapas?**\n",
    "- **Regiones claras**: Donde el modelo es muy confiado en su decisión\n",
    "- **Regiones de transición**: Donde el modelo es menos seguro\n",
    "- **Puntos superpuestos**: Nuestros datos reales para validar las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmaps-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapas de calor de algoritmos de detección: LOF y One-Class SVM\n",
    "\n",
    "def grid_scores(model_decision_fn, Z_data, steps=120):\n",
    "    \"\"\"\n",
    "    Crea una grilla 2D y calcula scores de decisión para visualización.\n",
    "    \n",
    "    Args:\n",
    "        model_decision_fn: Función de decisión del modelo\n",
    "        Z_data: Datos proyectados en 2D (PCA)\n",
    "        steps: Resolución de la grilla\n",
    "    \n",
    "    Returns:\n",
    "        xx, yy: Coordenadas de la grilla\n",
    "        zz: Scores de decisión en cada punto de la grilla\n",
    "    \"\"\"\n",
    "    # Definir límites de la grilla basados en los datos\n",
    "    x_min, x_max = Z_data[:,0].min()-1, Z_data[:,0].max()+1\n",
    "    y_min, y_max = Z_data[:,1].min()-1, Z_data[:,1].max()+1\n",
    "    \n",
    "    # Crear grilla uniforme\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, steps), \n",
    "                         np.linspace(y_min, y_max, steps))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Convertir de PCA 2D de vuelta a espacio 6D escalado\n",
    "    Xs_grid = pca.inverse_transform(grid)\n",
    "    \n",
    "    # Calcular scores de decisión y reformatear para visualización\n",
    "    zz = model_decision_fn(Xs_grid).reshape(xx.shape)\n",
    "    return xx, yy, zz\n",
    "\n",
    "# === LOCAL OUTLIER FACTOR (LOF) ===\n",
    "print(\"🔍 Generando mapa de calor para Local Outlier Factor...\")\n",
    "xx, yy, zz_lof = grid_scores(lambda Xs_: detector.lof.decision_function(Xs_), Z)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: LOF decision function\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contourf(xx, yy, zz_lof, levels=20, cmap='coolwarm', alpha=0.8)\n",
    "scatter = plt.scatter(Z[:,0], Z[:,1], c=y, cmap='bwr', s=20, \n",
    "                     edgecolor='white', linewidth=0.5, alpha=0.8)\n",
    "plt.colorbar(contour, label='LOF Score (+ = normal, - = outlier)')\n",
    "plt.title('LOF: Detección basada en Densidad Local', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# === ONE-CLASS SVM ===\n",
    "print(\"🎯 Generando mapa de calor para One-Class SVM...\")\n",
    "xx, yy, zz_svm = grid_scores(lambda Xs_: detector.svm.decision_function(Xs_), Z)\n",
    "\n",
    "# Subplot 2: One-Class SVM\n",
    "plt.subplot(1, 2, 2)\n",
    "contour = plt.contourf(xx, yy, zz_svm, levels=20, cmap='PiYG', alpha=0.8)\n",
    "scatter = plt.scatter(Z[:,0], Z[:,1], c=y, cmap='bwr', s=20, \n",
    "                     edgecolor='white', linewidth=0.5, alpha=0.8)\n",
    "plt.colorbar(contour, label='SVM Score (+ = normal, - = outlier)')\n",
    "plt.title('One-Class SVM: Frontera de Decisión', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de los modelos\n",
    "print(\"\\n📈 Estadísticas de los algoritmos:\")\n",
    "lof_scores = detector.lof.decision_function(Xs)\n",
    "svm_scores = detector.svm.decision_function(Xs)\n",
    "\n",
    "print(f\"LOF - Normal promedio: {lof_scores[:len(Xn)].mean():.3f}, \"\n",
    "      f\"Anómalo promedio: {lof_scores[len(Xn):].mean():.3f}\")\n",
    "print(f\"SVM - Normal promedio: {svm_scores[:len(Xn)].mean():.3f}, \"\n",
    "      f\"Anómalo promedio: {svm_scores[len(Xn):].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## 9. Pruebas de detección en tiempo real\n",
    "\n",
    "**¿Qué hace esta sección?**\n",
    "- Prueba el endpoint de detección del sistema con muestras específicas\n",
    "- Compara una muestra normal vs una muestra anómala\n",
    "- Muestra la respuesta completa del sistema incluyendo confianza y tipos de amenaza\n",
    "\n",
    "**Componentes de la respuesta:**\n",
    "- **threat_detected**: Boolean indicando si se detectó amenaza\n",
    "- **confidence**: Nivel de confianza (0.0-1.0)\n",
    "- **threat_types**: Lista de tipos de amenaza detectados\n",
    "- **scores**: Detalles de puntuación por método (reglas + ML)\n",
    "\n",
    "**Tipos de amenaza que puede detectar:**\n",
    "- `ddos`: Ataques de denegación de servicio\n",
    "- `port_scan`: Escaneo de puertos\n",
    "- `data_exfil`: Exfiltración de datos\n",
    "- `ml_high_risk`, `ml_medium_risk`: Clasificaciones por ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "testing-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas de detección usando el endpoint interno del detector\n",
    "\n",
    "# Seleccionar muestras representativas para testing\n",
    "test_normal = normal[0]      # Primera muestra normal\n",
    "test_anomaly = anoms[0]      # Primera muestra anómala\n",
    "\n",
    "samples = [\n",
    "    (\"🟢 TRÁFICO NORMAL\", test_normal),\n",
    "    (\"🔴 TRÁFICO ANÓMALO\", test_anomaly)\n",
    "]\n",
    "\n",
    "print(\"🧪 PRUEBAS DE DETECCIÓN EN TIEMPO REAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label, sample in samples:\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Mostrar métricas de entrada\n",
    "    print(\"📊 Métricas de entrada:\")\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Ejecutar detección\n",
    "    result = detector.detect(sample)\n",
    "    \n",
    "    # Mostrar resultado de detección\n",
    "    print(f\"\\n🎯 Resultado de detección:\")\n",
    "    print(f\"  Amenaza detectada: {'SÍ' if result['threat_detected'] else 'NO'}\")\n",
    "    print(f\"  Nivel de confianza: {result['confidence']:.2f}\")\n",
    "    \n",
    "    if result['threat_types']:\n",
    "        print(f\"  Tipos de amenaza: {', '.join(result['threat_types'])}\")\n",
    "    \n",
    "    if result['scores']:\n",
    "        print(f\"  Scores detallados:\")\n",
    "        for method, score in result['scores'].items():\n",
    "            print(f\"    {method}: {score}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Análisis de rendimiento rápido\n",
    "print(\"\\n📈 ANÁLISIS DE RENDIMIENTO RÁPIDO\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Probar con más muestras para estadísticas\n",
    "test_samples = normal[:10] + anoms[:10]\n",
    "test_labels = [0]*10 + [1]*10\n",
    "\n",
    "correct_detections = 0\n",
    "for i, (sample, expected) in enumerate(zip(test_samples, test_labels)):\n",
    "    result = detector.detect(sample)\n",
    "    detected = 1 if result['threat_detected'] else 0\n",
    "    if detected == expected:\n",
    "        correct_detections += 1\n",
    "\n",
    "accuracy = correct_detections / len(test_samples)\n",
    "print(f\"Precisión en muestra pequeña: {accuracy:.2f} ({correct_detections}/{len(test_samples)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-section",
   "metadata": {},
   "source": [
    "## 🎉 Conclusiones del análisis\n",
    "\n",
    "### Lo que hemos aprendido:\n",
    "\n",
    "1. **Separabilidad de datos**: Los algoritmos pueden distinguir entre tráfico normal y anómalo\n",
    "2. **Efectividad de modelos**: Cada algoritmo (KMeans, LOF, SVM) aporta una perspectiva diferente\n",
    "3. **Visualización PCA**: Permite entender cómo \"ve\" el sistema los patrones de tráfico\n",
    "4. **Sistema híbrido**: Combina reglas determinísticas con machine learning para mayor precisión\n",
    "\n",
    "### Próximos pasos para mejorar:\n",
    "\n",
    "- **Datos reales**: Reemplazar datos sintéticos con tráfico real capturado por eBPF\n",
    "- **Ajuste de hiperparámetros**: Optimizar configuraciones de los modelos\n",
    "- **Nuevas características**: Agregar métricas adicionales (latencia, protocolos, etc.)\n",
    "- **Evaluación completa**: Métricas de precision, recall, F1-score con datasets más grandes\n",
    "\n",
    "### Uso en producción:\n",
    "\n",
    "El detector entrenado se integra con:\n",
    "- **eBPF Monitor**: Captura métricas de red en tiempo real\n",
    "- **API REST**: Endpoint `/detect` para evaluación de tráfico\n",
    "- **Prometheus**: Métricas de rendimiento y alertas\n",
    "- **Grafana**: Dashboards de visualización y monitoreo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}